{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQmwC3DGYBpJBvIiGc8Zry"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22kbEcBGu9vY","executionInfo":{"status":"ok","timestamp":1670389476552,"user_tz":480,"elapsed":1625,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}},"outputId":"519c7b23-6393-48dc-9058-dcb168157734"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from google.colab import files"],"metadata":{"id":"yCDZf-UGzeYi","executionInfo":{"status":"ok","timestamp":1670389478762,"user_tz":480,"elapsed":179,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#path where the csv is located\n","path = \"/content/drive/MyDrive/\""],"metadata":{"id":"Zz4MQFBoznkU","executionInfo":{"status":"ok","timestamp":1670389481830,"user_tz":480,"elapsed":160,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# import all packagees\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.naive_bayes import MultinomialNB\n","\n","import nltk # it is the one of most comman libraries for Natural Language Process\n","import re # Regular Expression library"],"metadata":{"id":"FRkKkgb0zpdS","executionInfo":{"status":"ok","timestamp":1670389486236,"user_tz":480,"elapsed":2384,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# importing the csv file from google drive\n","with open(path + \"gender-classifier-data.csv\", encoding=\"latin1\", errors='ignore') as f:\n","  twitter_data = pd.read_csv(f)"],"metadata":{"id":"bUZnWPmkzr5T","executionInfo":{"status":"ok","timestamp":1670389508419,"user_tz":480,"elapsed":339,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#Use only the gender and text column\n","data = pd.concat([twitter_data.gender, twitter_data.text], axis=1)"],"metadata":{"id":"Sdltrk1fz51m","executionInfo":{"status":"ok","timestamp":1670389510342,"user_tz":480,"elapsed":141,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#Drop Brand\n","data = data[data[\"gender\"] != \"brand\"]\n","data = data[data[\"gender\"] != \"unknown\"]"],"metadata":{"id":"teOwL5KWz89I","executionInfo":{"status":"ok","timestamp":1670389520068,"user_tz":480,"elapsed":169,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import nltk # natural language took kit\n","nltk.download(\"stopwords\")\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDk8_ou_z9yg","executionInfo":{"status":"ok","timestamp":1670389530034,"user_tz":480,"elapsed":336,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}},"outputId":"41152d0f-7c18-460c-ab46-ae71573797bd"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# genders have two options (male/female). so i changed male to 0 and female to 1.\n","data.gender = [1 if gender == \"female\" else 0 for gender in data.gender]"],"metadata":{"id":"eeFDTSAe0F5F","executionInfo":{"status":"ok","timestamp":1670389533297,"user_tz":480,"elapsed":139,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["descript_list = []\n","for descript in data.text:\n","    descript = re.sub(\"[^a-zA-Z]\", \" \", descript)\n","    descript= descript.lower()\n","    descript = nltk.word_tokenize(descript)\n","    lemma = nltk.WordNetLemmatizer()\n","    descript = [lemma.lemmatize(word) for word in descript]\n","    descript = \" \".join(descript)\n","    descript_list.append(descript)"],"metadata":{"id":"YwUNuRxO0KKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TF-IDF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","max_features = 5000\n","ti = TfidfVectorizer(stop_words = \"english\", ngram_range=(1, 2))\n","sparce_matrix_ti = ti.fit_transform(descript_list).toarray()\n","print(\"top used {} words: {}\".format(max_features, ti.get_feature_names()))"],"metadata":{"id":"tDUOMqqe0r01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get column 0, which is the gender as an NDARRAY of shape ( n,) \n","y = data.iloc[:, 0].values\n","x = sparce_matrix_ti"],"metadata":{"id":"hUviIXwI00MA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif,f_classif\n","feature_selector = SelectKBest(score_func=chi2, k=1000).fit(sparce_matrix_ti, y)\n","# get best feature names\n","important_feature_names = feature_selector.get_feature_names_out(ti.get_feature_names_out())\n","name_and_scores = list(zip(important_feature_names, feature_selector.scores_))\n","name_and_scores.sort(key=lambda x: x[1], reverse=True)\n","for ns in name_and_scores:\n","  print(ns)"],"metadata":{"id":"G8X33NG403ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now, we will implement \"bag of words\" method\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","max_features = 5000 # we will take top 5000 feature,means it will give 5000 most common words \n","\n","cv = CountVectorizer(max_features=max_features, stop_words = \"english\")\n","# in this method, we remove the stopwords (irrelevant words) in English language. (like \"of\", \"and\", \"the\" etc.)\n","\n","sparce_matrix = cv.fit_transform(descript_list).toarray()\n","\n","print(\"top used {} words: {}\".format(max_features, cv.get_feature_names()))"],"metadata":{"id":"rayUo3BT1Nwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get column 0, which is the gender as an NDARRAY of shape ( n,) \n","y = data.iloc[:, 0].values\n","x = sparce_matrix_ti\n","print(sparce_matrix_ti)"],"metadata":{"id":"5_fGASyV1Rns","colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"status":"error","timestamp":1670388618075,"user_tz":480,"elapsed":153,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}},"outputId":"be5acf72-5f9b-40a3-f9e4-3fb0831df38f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-45b0a6449ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get column 0, which is the gender as an NDARRAY of shape ( n,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparce_matrix_ti\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparce_matrix_ti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]},{"cell_type":"code","source":["# cross validation, use 10% of the data to test;42 is a common choice to decide the seed of randomness\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(feature_selector.transform(sparce_matrix_ti), y, test_size = 0.1, random_state = 42)\n","print(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"ImKGIkyj1Vt5","executionInfo":{"status":"error","timestamp":1670388938904,"user_tz":480,"elapsed":159,"user":{"displayName":"Taylor D'Ambrosio","userId":"07377749190308305520"}},"outputId":"e2b27ef1-b4c2-4ad4-c5f2-aed04f34c1a3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-85e7f4018436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparce_matrix_ti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'feature_selector' is not defined"]}]},{"cell_type":"code","source":["#Import svm model\n","from sklearn import svm\n","from sklearn import metrics\n","\n","#Classifier\n","svm_classifier = svm.SVC(kernel='linear') # Linear Kernel\n","\n","#Train\n","svm_classifier.fit(x_train, y_train)\n","\n","#Predict\n","y_pred = svm_classifier.predict(x_test)\n","\n","# SVM accuacy\n","print(\"SVM Accuracy (text):\",metrics.accuracy_score(y_test, y_pred))"],"metadata":{"id":"h5KGOod-1Zfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use svm on description col\n","data_descript = pd.concat([twitter_data.gender, twitter_data.description], axis=1)\n"],"metadata":{"id":"Yd4dnMqn1nNR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Drop Brand and unknown\n","data_descript = data_descript[data_descript[\"gender\"] != \"brand\"]\n","data_descript = data[data[\"gender\"] != \"unknown\"]"],"metadata":{"id":"8CpmvnFr2Vc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# genders have two options (male/female). so i changed male to 0 and female to 1.\n","data_descript.gender = [1 if gender == \"female\" else 0 for gender in data.gender]"],"metadata":{"id":"U54fkw2x2aI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","descript_list = []\n","for descript in data_descript.descript:\n","    descript = re.sub(\"[^a-zA-Z]\", \" \", descript)\n","    descript = descript.lower()\n","    descript = nltk.word_tokenize(descript)\n","    lemma = nltk.WordNetLemmatizer()\n","    descript = [lemma.lemmatize(word) for word in descript]\n","    descript = \" \".join(descript)\n","    descript_list.append(descript)"],"metadata":{"id":"4Psvwmrf3IZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TF-IDF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","max_features = 5000\n","#vectorizer = TfidfVectorizer()\n","# ti = TfidfVectorizer(max_features = max_features, stop_words = \"english\")\n","ti = TfidfVectorizer(stop_words = \"english\", ngram_range=(1, 2))\n","sparce_matrix_ti = ti.fit_transform(description_list).toarray()\n","print(\"top used {} words: {}\".format(max_features, ti.get_feature_names()))"],"metadata":{"id":"g2oV-Lkr3i5u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = data.iloc[:, 0].values\n","x = sparce_matrix_ti"],"metadata":{"id":"6SvS-RPa3otV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature extraction\n","from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif,f_classif\n","# selected_features = SelectKBest(score_func=chi2, k=1000).fit_transform(sparce_matrix_ti, y)\n","feature_selector = SelectKBest(score_func=chi2, k=1000).fit(sparce_matrix_ti, y)\n","# get best feature names\n","important_feature_names = feature_selector.get_feature_names_out(ti.get_feature_names_out())\n","name_and_scores = list(zip(important_feature_names, feature_selector.scores_))\n","name_and_scores.sort(key=lambda x: x[1], reverse=True)\n","#for ns in name_and_scores:\n","#  print(ns)"],"metadata":{"id":"bOF069hH3pcI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","max_features = 5000 # we will take top 5000 feature,means it will give 5000 most common words \n","\n","cv = CountVectorizer(max_features=max_features, stop_words = \"english\")\n","# in this method, we remove the stopwords (irrelevant words) in English language. (like \"of\", \"and\", \"the\" etc.)\n","\n","sparce_matrix = cv.fit_transform(description_list).toarray()\n","\n","print(\"top used {} words: {}\".format(max_features, cv.get_feature_names()))"],"metadata":{"id":"gdqbNVXD3zRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = data.iloc[:, 0].values\n","x = sparce_matrix_ti\n","print(sparce_matrix_ti)"],"metadata":{"id":"Qu3LxRjZ32Cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cross validation, use 10% of the data to test;42 is a common choice to decide the seed of randomness\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(feature_selector.transform(sparce_matrix_ti), y, test_size = 0.1, random_state = 42)\n","print(y_test)"],"metadata":{"id":"bdTYFXZ136Lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Import svm model\n","from sklearn import svm\n","from sklearn import metrics\n","\n","#Classifier\n","svm_classifier = svm.SVC(kernel='linear') # Linear Kernel\n","\n","#Train\n","svm_classifier.fit(x_train, y_train)\n","\n","#Predict\n","y_pred = svm_classifier.predict(x_test)\n","\n","# SVM accuacy\n","print(\"SVM Accuracy (description):\",metrics.accuracy_score(y_test, y_pred))"],"metadata":{"id":"1VHubneB3-Uo"},"execution_count":null,"outputs":[]}]}